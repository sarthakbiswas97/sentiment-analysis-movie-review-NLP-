{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef434f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "basepath = '/Users/sarthakbiswas/Downloads/aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n",
    "\n",
    "records = []\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file),\n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            records.append([txt, labels[l]])\n",
    "            pbar.update()\n",
    "\n",
    "df = pd.DataFrame(records, columns=['review', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d47ef2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0677bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "# column renaming\n",
    "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f40c265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96085746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n",
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['The sun is shining', 'The weather is sweet', 'The sun is shining and the weather is sweet'])\n",
    "\n",
    "bag = count.fit_transform(docs)\n",
    "print(count.vocabulary_)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a65c0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.56 0.56 0.   0.43 0.  ]\n",
      " [0.   0.43 0.   0.   0.56 0.43 0.56]\n",
      " [0.4  0.48 0.31 0.31 0.31 0.48 0.31]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# used for transformation of the bag of words( count matrix from count vectorizer) into a tf-idf representation\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(bag).toarray()\n",
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b671c",
   "metadata": {},
   "source": [
    "cleaning text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99ad779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  last 50 characters from the first document \n",
    "\n",
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee5d9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/var/folders/8t/5rdykr5x1sb_n85_5z0tjcr80000gn/T/ipykernel_37117/124701657.py:6: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
      "/var/folders/8t/5rdykr5x1sb_n85_5z0tjcr80000gn/T/ipykernel_37117/124701657.py:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  text = (re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', ''))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    # remove html tags\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # remove emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    # remove non-word characters and convert to lowercase\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e39ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  just a check \n",
    "preprocessor(df.loc[0, 'review'][-50:])\n",
    "preprocessor(\"</a>This :) is :( a test :-)!\")\n",
    "\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64623a",
   "metadata": {},
   "source": [
    "processing documents into tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457cda11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# porter stemming algorithm \n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer = tokenizer_porter(\"runners like running and thus they run\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce4c72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words \n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "stop = ENGLISH_STOP_WORDS\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a15ce5",
   "metadata": {},
   "source": [
    "training logistic regression model for document classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dcc9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  split the data into training and testing sets \n",
    "\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a876fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run']; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run']; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run']; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run']; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run']; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run']; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run']; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run']; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run']; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run']; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x12ca15f80>; total time= 2.0min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11aafa700>; total time= 2.0min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x131bda700>; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x144fba700>; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x13cb5df80>; total time= 2.0min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'con', 'beforehand', 'show', 'for', 'over', 'empty', 'more', 'thin', 'four', 'towards', 'herein', 'hence', 'until', 'among', 'will', 'become', 'former', 'next', 'thick', 'whole', 'whereas', 'never', 'hereafter', 'everything', 'is', 'our', 'during', 'while', 'than', 'their', 'it', 'others', 'that', 'down', 'latterly', 'fill', 'herself', 'whenever', 'his', 'ltd', 'eg', 'several', 'side', 'since', 'my', 'your', 'see', 'up', 'when', 'are', 'becomes', 'every', 'were', 'everywhere', 'would', 'indeed', 'yet', 'interest', 'ten', 'namely', 'elsewhere', 'anything', 'couldnt', 'seemed', 'of', 'name', 'you', 'becoming', 'move', 'very', 'third', 'nine', 'whom', 'around', 'sometime', 'beyond', 'wherever', 'twenty', 'seems', 'too', 'below', 'forty', 'between', 'who', 'most', 'thence', 'behind', 'etc', 'otherwise', 'once', 'have', 'those', 'above', 'be', 'we', 'only', 'to', 'alone', 'off', 'full', 'nobody', 'hasnt', 'eight', 'he', 'hundred', 'un', 'fifteen', 'became', 'before', 'first', 'one', 'seem', 'the', 'ie', 'am', 'whose', 'some', 're', 'whence', 'thereupon', 'now', 'can', 'meanwhile', 'call', 'whereby', 'itself', 'however', 'being', 'then', 'enough', 'last', 'as', 'within', 'on', 'none', 'still', 'almost', 'co', 'thus', 'across', 'many', 'besides', 'moreover', 'must', 'under', 'also', 'anywhere', 'hereby', 'hers', 'get', 'bill', 'twelve', 'even', 'amoungst', 'which', 'was', 'do', 'well', 'de', 'they', 'himself', 'these', 'perhaps', 'serious', 'her', 'us', 'its', 'without', 'if', 'might', 'how', 'back', 'someone', 'why', 'ourselves', 'whoever', 'fifty', 'whereafter', 'other', 'neither', 'themselves', 'but', 'via', 'an', 'or', 'each', 'yours', 'hereupon', 'six', 'nothing', 'any', 'either', 'not', 'nevertheless', 'where', 'rather', 'put', 'detail', 'may', 'along', 'nowhere', 'made', 'should', 'anyway', 'has', 'sixty', 'system', 'done', 'anyhow', 'what', 'cry', 'latter', 'both', 'three', 'she', 'found', 'in', 'often', 'therein', 'beside', 'all', 'therefore', 'much', 'sometimes', 'out', 'top', 'per', 'by', 'noone', 'give', 'part', 'least', 'ours', 'here', 'cant', 'and', 'yourselves', 'else', 'so', 'yourself', 'into', 'due', 'mill', 'with', 'a', 'me', 'upon', 'after', 'together', 'anyone', 'always', 'mine', 'thru', 'again', 'had', 'myself', 'thereby', 'against', 'although', 'two', 'whether', 'amongst', 'been', 'bottom', 'afterwards', 'own', 'ever', 'throughout', 'nor', 'whereupon', 'another', 'find', 'except', 'this', 'please', 'somewhere', 'seeming', 'five', 'go', 'toward', 'at', 'take', 'because', 'fire', 'thereafter', 'mostly', 'somehow', 'no', 'keep', 'amount', 'them', 'whatever', 'everyone', 'i', 'eleven', 'whither', 'though', 'formerly', 'wherein', 'describe', 'same', 'onto', 'through', 'further', 'from', 'sincere', 'less', 'there', 'him', 'inc', 'about', 'already', 'such', 'something', 'could', 'cannot', 'few', 'front'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'there', 'becomes', 'became', 'top', 'without', 'onto', 'namely', 'hers', 'more', 'behind', 'whose', 'bottom', 'have', 'into', 'his', 'are', 'than', 'that', 'who', 'fire', 'such', 'empty', 'how', 'five', 'per', 'beside', 'thick', 'name', 'should', 'first', 'after', 'whither', 'therein', 'had', 'few', 'nothing', 'every', 'others', 'fifty', 'yourselves', 'must', 'your', 'no', 'done', 'forty', 'now', 'her', 'everywhere', 'i', 'on', 'toward', 'thereupon', 'several', 'ten', 'my', 'same', 'almost', 'hereby', 'back', 'inc', 'show', 'throughout', 'yet', 'is', 'within', 'mill', 'describe', 'front', 'both', 'keep', 'four', 'these', 'next', 'might', 'together', 'herself', 'whence', 'besides', 'above', 'nobody', 'take', 'found', 'mostly', 'everyone', 'were', 'otherwise', 'co', 'please', 'except', 'three', 'however', 'via', 'upon', 'our', 'it', 'enough', 'to', 'yourself', 'wherein', 'during', 'we', 'seeming', 'therefore', 'find', 'many', 'because', 'hasnt', 'ourselves', 'whoever', 'wherever', 'six', 'own', 'move', 'may', 'thru', 'would', 'something', 'further', 'yours', 'whereby', 'see', 'but', 'amoungst', 'where', 'most', 'under', 'get', 'sometimes', 'hereafter', 'con', 'least', 'anywhere', 'has', 'un', 'ltd', 'them', 'noone', 'then', 'us', 'meanwhile', 'whatever', 'anyhow', 'well', 'while', 'so', 'themselves', 'down', 'you', 'twelve', 'some', 'too', 'rather', 'moreover', 'if', 'seem', 'afterwards', 'along', 'eight', 'seemed', 'any', 'cant', 'between', 'this', 'do', 'whereafter', 'already', 'whom', 'once', 'beyond', 'beforehand', 'myself', 'the', 'formerly', 'thence', 'sincere', 'though', 'am', 'elsewhere', 'from', 'ever', 'perhaps', 'call', 'again', 'fifteen', 'what', 'sometime', 'thereby', 'interest', 'fill', 're', 'former', 'him', 'they', 'less', 'not', 'everything', 'nevertheless', 'across', 'always', 'up', 'or', 'off', 'before', 'by', 'made', 'alone', 'thin', 'system', 'other', 'me', 'give', 'cannot', 'due', 'below', 'eg', 'neither', 'of', 'can', 'for', 'also', 'twenty', 'an', 'among', 'even', 'much', 'being', 'could', 'hence', 'with', 'himself', 'someone', 'thus', 'amongst', 'when', 'through', 'one', 'couldnt', 'each', 'against', 'becoming', 'in', 'whenever', 'will', 'anyone', 'detail', 'latterly', 'somehow', 'thereafter', 'never', 'de', 'nor', 'only', 'full', 'here', 'was', 'put', 'still', 'somewhere', 'and', 'he', 'often', 'go', 'serious', 'seems', 'she', 'itself', 'last', 'their', 'all', 'cry', 'since', 'part', 'ie', 'nowhere', 'whether', 'as', 'another', 'at', 'towards', 'hereupon', 'whole', 'whereupon', 'side', 'those', 'out', 'latter', 'hundred', 'etc', 'anyway', 'indeed', 'bill', 'been', 'amount', 'its', 'be', 'which', 'third', 'become', 'why', 'nine', 'although', 'over', 'about', 'herein', 'around', 'anything', 'eleven', 'very', 'sixty', 'none', 'two', 'until', 'ours', 'mine', 'either', 'whereas', 'a', 'else'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'same', 'or', 'see', 'whatever', 'a', 'they', 'than', 'hereafter', 'meanwhile', 'if', 'over', 'whence', 'again', 'well', 'cannot', 'bill', 'ours', 'whither', 'everywhere', 'along', 'move', 'ever', 'off', 'twenty', 'interest', 'against', 'one', 'keep', 'any', 'sixty', 'three', 'cry', 'all', 'could', 'anyone', 'also', 'anyhow', 'although', 'him', 'next', 'upon', 'none', 'up', 'serious', 'another', 'my', 'de', 'themselves', 'how', 'everything', 'thru', 'fill', 'your', 'myself', 'forty', 'an', 'himself', 'four', 'whoever', 'should', 'down', 'side', 'everyone', 'our', 'nowhere', 'seeming', 'third', 'eleven', 'by', 'since', 'whereas', 'within', 'seem', 'us', 'take', 'nevertheless', 'somehow', 'formerly', 'might', 'less', 'latterly', 'which', 'every', 'would', 'still', 'between', 'about', 'across', 'had', 'thick', 'through', 'fifteen', 'whereby', 'get', 'anything', 'find', 'whereafter', 'become', 'herein', 'several', 'seems', 'we', 'around', 'somewhere', 'mill', 'ten', 'that', 'besides', 'beforehand', 'co', 'hasnt', 'please', 'ie', 'without', 'amongst', 'once', 'made', 'you', 'under', 'in', 'rather', 'couldnt', 'only', 'hundred', 'do', 'always', 'here', 'afterwards', 'each', 'after', 'fifty', 'together', 'already', 'some', 'when', 'hence', 'never', 'this', 'system', 'throughout', 'two', 'both', 'onto', 'con', 'while', 'describe', 'go', 'noone', 'twelve', 'from', 'was', 'therein', 'because', 'among', 'etc', 'anywhere', 'yourself', 'may', 'for', 'bottom', 'former', 'per', 'became', 'indeed', 'much', 'beyond', 'yet', 'behind', 'eg', 'detail', 'thereby', 'least', 'most', 'ourselves', 'until', 'were', 'with', 'even', 'nobody', 'ltd', 'further', 'whole', 'his', 'amount', 'however', 'sometime', 'being', 'show', 'at', 'often', 'done', 'becomes', 'above', 'towards', 'itself', 'hereupon', 'either', 'of', 'something', 'and', 'have', 'no', 'whereupon', 'elsewhere', 'becoming', 'where', 'whenever', 'sincere', 'can', 'be', 'on', 'mine', 'yours', 'thereupon', 'but', 'hers', 'full', 'enough', 'otherwise', 'found', 'them', 'amoungst', 'these', 'yourselves', 'must', 'below', 'not', 'am', 'fire', 'it', 'he', 'perhaps', 'toward', 'thence', 'more', 'very', 'un', 'due', 'been', 'such', 'now', 'who', 'its', 'so', 'namely', 'sometimes', 'thereafter', 're', 'eight', 'me', 'other', 'those', 'are', 'nothing', 'whose', 'give', 'the', 'inc', 'seemed', 'someone', 'put', 'i', 'alone', 'latter', 'what', 'many', 'out', 'thin', 'whom', 'except', 'herself', 'will', 'first', 'beside', 'she', 'therefore', 'else', 'her', 'cant', 'during', 'few', 'via', 'why', 'wherein', 'is', 'as', 'last', 'though', 'moreover', 'whether', 'call', 'others', 'mostly', 'empty', 'part', 'hereby', 'own', 'wherever', 'nine', 'five', 'their', 'top', 'nor', 'before', 'into', 'back', 'name', 'almost', 'too', 'front', 'then', 'six', 'neither', 'to', 'anyway', 'there', 'has', 'thus'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'there', 'becomes', 'became', 'top', 'without', 'onto', 'namely', 'hers', 'more', 'behind', 'whose', 'bottom', 'have', 'into', 'his', 'are', 'than', 'that', 'who', 'fire', 'such', 'empty', 'how', 'five', 'per', 'beside', 'thick', 'name', 'should', 'first', 'after', 'whither', 'therein', 'had', 'few', 'nothing', 'every', 'others', 'fifty', 'yourselves', 'must', 'your', 'no', 'done', 'forty', 'now', 'her', 'everywhere', 'i', 'on', 'toward', 'thereupon', 'several', 'ten', 'my', 'same', 'almost', 'hereby', 'back', 'inc', 'show', 'throughout', 'yet', 'is', 'within', 'mill', 'describe', 'front', 'both', 'keep', 'four', 'these', 'next', 'might', 'together', 'herself', 'whence', 'besides', 'above', 'nobody', 'take', 'found', 'mostly', 'everyone', 'were', 'otherwise', 'co', 'please', 'except', 'three', 'however', 'via', 'upon', 'our', 'it', 'enough', 'to', 'yourself', 'wherein', 'during', 'we', 'seeming', 'therefore', 'find', 'many', 'because', 'hasnt', 'ourselves', 'whoever', 'wherever', 'six', 'own', 'move', 'may', 'thru', 'would', 'something', 'further', 'yours', 'whereby', 'see', 'but', 'amoungst', 'where', 'most', 'under', 'get', 'sometimes', 'hereafter', 'con', 'least', 'anywhere', 'has', 'un', 'ltd', 'them', 'noone', 'then', 'us', 'meanwhile', 'whatever', 'anyhow', 'well', 'while', 'so', 'themselves', 'down', 'you', 'twelve', 'some', 'too', 'rather', 'moreover', 'if', 'seem', 'afterwards', 'along', 'eight', 'seemed', 'any', 'cant', 'between', 'this', 'do', 'whereafter', 'already', 'whom', 'once', 'beyond', 'beforehand', 'myself', 'the', 'formerly', 'thence', 'sincere', 'though', 'am', 'elsewhere', 'from', 'ever', 'perhaps', 'call', 'again', 'fifteen', 'what', 'sometime', 'thereby', 'interest', 'fill', 're', 'former', 'him', 'they', 'less', 'not', 'everything', 'nevertheless', 'across', 'always', 'up', 'or', 'off', 'before', 'by', 'made', 'alone', 'thin', 'system', 'other', 'me', 'give', 'cannot', 'due', 'below', 'eg', 'neither', 'of', 'can', 'for', 'also', 'twenty', 'an', 'among', 'even', 'much', 'being', 'could', 'hence', 'with', 'himself', 'someone', 'thus', 'amongst', 'when', 'through', 'one', 'couldnt', 'each', 'against', 'becoming', 'in', 'whenever', 'will', 'anyone', 'detail', 'latterly', 'somehow', 'thereafter', 'never', 'de', 'nor', 'only', 'full', 'here', 'was', 'put', 'still', 'somewhere', 'and', 'he', 'often', 'go', 'serious', 'seems', 'she', 'itself', 'last', 'their', 'all', 'cry', 'since', 'part', 'ie', 'nowhere', 'whether', 'as', 'another', 'at', 'towards', 'hereupon', 'whole', 'whereupon', 'side', 'those', 'out', 'latter', 'hundred', 'etc', 'anyway', 'indeed', 'bill', 'been', 'amount', 'its', 'be', 'which', 'third', 'become', 'why', 'nine', 'although', 'over', 'about', 'herein', 'around', 'anything', 'eleven', 'very', 'sixty', 'none', 'two', 'until', 'ours', 'mine', 'either', 'whereas', 'a', 'else'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'same', 'or', 'see', 'whatever', 'a', 'they', 'than', 'hereafter', 'meanwhile', 'if', 'over', 'whence', 'again', 'well', 'cannot', 'bill', 'ours', 'whither', 'everywhere', 'along', 'move', 'ever', 'off', 'twenty', 'interest', 'against', 'one', 'keep', 'any', 'sixty', 'three', 'cry', 'all', 'could', 'anyone', 'also', 'anyhow', 'although', 'him', 'next', 'upon', 'none', 'up', 'serious', 'another', 'my', 'de', 'themselves', 'how', 'everything', 'thru', 'fill', 'your', 'myself', 'forty', 'an', 'himself', 'four', 'whoever', 'should', 'down', 'side', 'everyone', 'our', 'nowhere', 'seeming', 'third', 'eleven', 'by', 'since', 'whereas', 'within', 'seem', 'us', 'take', 'nevertheless', 'somehow', 'formerly', 'might', 'less', 'latterly', 'which', 'every', 'would', 'still', 'between', 'about', 'across', 'had', 'thick', 'through', 'fifteen', 'whereby', 'get', 'anything', 'find', 'whereafter', 'become', 'herein', 'several', 'seems', 'we', 'around', 'somewhere', 'mill', 'ten', 'that', 'besides', 'beforehand', 'co', 'hasnt', 'please', 'ie', 'without', 'amongst', 'once', 'made', 'you', 'under', 'in', 'rather', 'couldnt', 'only', 'hundred', 'do', 'always', 'here', 'afterwards', 'each', 'after', 'fifty', 'together', 'already', 'some', 'when', 'hence', 'never', 'this', 'system', 'throughout', 'two', 'both', 'onto', 'con', 'while', 'describe', 'go', 'noone', 'twelve', 'from', 'was', 'therein', 'because', 'among', 'etc', 'anywhere', 'yourself', 'may', 'for', 'bottom', 'former', 'per', 'became', 'indeed', 'much', 'beyond', 'yet', 'behind', 'eg', 'detail', 'thereby', 'least', 'most', 'ourselves', 'until', 'were', 'with', 'even', 'nobody', 'ltd', 'further', 'whole', 'his', 'amount', 'however', 'sometime', 'being', 'show', 'at', 'often', 'done', 'becomes', 'above', 'towards', 'itself', 'hereupon', 'either', 'of', 'something', 'and', 'have', 'no', 'whereupon', 'elsewhere', 'becoming', 'where', 'whenever', 'sincere', 'can', 'be', 'on', 'mine', 'yours', 'thereupon', 'but', 'hers', 'full', 'enough', 'otherwise', 'found', 'them', 'amoungst', 'these', 'yourselves', 'must', 'below', 'not', 'am', 'fire', 'it', 'he', 'perhaps', 'toward', 'thence', 'more', 'very', 'un', 'due', 'been', 'such', 'now', 'who', 'its', 'so', 'namely', 'sometimes', 'thereafter', 're', 'eight', 'me', 'other', 'those', 'are', 'nothing', 'whose', 'give', 'the', 'inc', 'seemed', 'someone', 'put', 'i', 'alone', 'latter', 'what', 'many', 'out', 'thin', 'whom', 'except', 'herself', 'will', 'first', 'beside', 'she', 'therefore', 'else', 'her', 'cant', 'during', 'few', 'via', 'why', 'wherein', 'is', 'as', 'last', 'though', 'moreover', 'whether', 'call', 'others', 'mostly', 'empty', 'part', 'hereby', 'own', 'wherever', 'nine', 'five', 'their', 'top', 'nor', 'before', 'into', 'back', 'name', 'almost', 'too', 'front', 'then', 'six', 'neither', 'to', 'anyway', 'there', 'has', 'thus'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x12f09e7a0>; total time= 2.0min\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'ever', 'thence', 'others', 'formerly', 'call', 'somewhere', 'twenty', 'might', 'last', 'hence', 'wherever', 'about', 'anywhere', 'everywhere', 'can', 'neither', 'towards', 'front', 'cannot', 'both', 'once', 'himself', 'five', 'latter', 'via', 'another', 'our', 'amongst', 'which', 'above', 'us', 'while', 'part', 'whether', 'every', 'six', 'yourself', 'been', 'under', 'such', 'together', 'else', 'hundred', 'made', 'afterwards', 'whatever', 'give', 'go', 'three', 'however', 'for', 'though', 'none', 'may', 'themselves', 'other', 'seemed', 'top', 'now', 'thereby', 'or', 'sincere', 'are', 'any', 'describe', 'they', 'then', 'everything', 'further', 'amount', 'two', 'whole', 'it', 'whereupon', 'keep', 'per', 'nevertheless', 'but', 'of', 'she', 'see', 'several', 'myself', 'cry', 'many', 'hereafter', 'although', 'bill', 'sixty', 'latterly', 'done', 'former', 'perhaps', 'against', 'due', 'side', 'hasnt', 'even', 'who', 'than', 'around', 'with', 'name', 'across', 'him', 'is', 'by', 'always', 'whoever', 'his', 'much', 'all', 'anyway', 'toward', 'fifteen', 'anyhow', 'has', 'would', 'seeming', 'anyone', 'therein', 'full', 'please', 'am', 'beforehand', 'serious', 're', 'nine', 'same', 'enough', 'how', 'often', 'inc', 'again', 'do', 'because', 'anything', 'alone', 'get', 'them', 'between', 'ourselves', 'become', 'had', 'move', 'those', 'very', 'and', 'i', 'system', 'thin', 'seems', 'up', 'in', 'a', 'ie', 'so', 'next', 'beside', 'until', 'something', 'moreover', 'first', 'without', 'off', 'among', 'nobody', 'found', 'whereafter', 'becoming', 'few', 'mine', 'you', 'me', 'twelve', 'interest', 'he', 'beyond', 'also', 'take', 'behind', 'these', 'least', 'well', 'four', 'meanwhile', 'someone', 'everyone', 'etc', 'still', 'thru', 'this', 'most', 'sometime', 'find', 'detail', 'therefore', 'whereby', 'except', 'eight', 'be', 'nothing', 'to', 'before', 'thereafter', 'whenever', 'bottom', 'hereby', 'within', 'co', 'wherein', 'eg', 'being', 'their', 'itself', 'hereupon', 'what', 'ours', 'thick', 'mill', 'each', 'noone', 'one', 'should', 'when', 'no', 'forty', 'namely', 'will', 'the', 'here', 'onto', 'besides', 'your', 'on', 'whereas', 'whose', 'de', 'why', 'fill', 'if', 'herein', 'yet', 'con', 'seem', 'more', 'somehow', 'rather', 'must', 'elsewhere', 'an', 'too', 'into', 'was', 'were', 'show', 'we', 'after', 'cant', 'since', 'through', 'never', 'not', 'fire', 'during', 'ten', 'yours', 'could', 'empty', 'from', 'put', 'thus', 'whom', 'became', 'whither', 'thereupon', 'less', 'un', 'indeed', 'mostly', 'throughout', 'out', 'own', 'nor', 'hers', 'where', 'that', 'back', 'some', 'only', 'my', 'sometimes', 'third', 'upon', 'herself', 'already', 'fifty', 'as', 'its', 'nowhere', 'ltd', 'yourselves', 'otherwise', 'almost', 'at', 'there', 'down', 'becomes', 'couldnt', 'either', 'amoungst', 'over', 'eleven', 'have', 'along', 'below', 'her', 'whence'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'same', 'or', 'see', 'whatever', 'a', 'they', 'than', 'hereafter', 'meanwhile', 'if', 'over', 'whence', 'again', 'well', 'cannot', 'bill', 'ours', 'whither', 'everywhere', 'along', 'move', 'ever', 'off', 'twenty', 'interest', 'against', 'one', 'keep', 'any', 'sixty', 'three', 'cry', 'all', 'could', 'anyone', 'also', 'anyhow', 'although', 'him', 'next', 'upon', 'none', 'up', 'serious', 'another', 'my', 'de', 'themselves', 'how', 'everything', 'thru', 'fill', 'your', 'myself', 'forty', 'an', 'himself', 'four', 'whoever', 'should', 'down', 'side', 'everyone', 'our', 'nowhere', 'seeming', 'third', 'eleven', 'by', 'since', 'whereas', 'within', 'seem', 'us', 'take', 'nevertheless', 'somehow', 'formerly', 'might', 'less', 'latterly', 'which', 'every', 'would', 'still', 'between', 'about', 'across', 'had', 'thick', 'through', 'fifteen', 'whereby', 'get', 'anything', 'find', 'whereafter', 'become', 'herein', 'several', 'seems', 'we', 'around', 'somewhere', 'mill', 'ten', 'that', 'besides', 'beforehand', 'co', 'hasnt', 'please', 'ie', 'without', 'amongst', 'once', 'made', 'you', 'under', 'in', 'rather', 'couldnt', 'only', 'hundred', 'do', 'always', 'here', 'afterwards', 'each', 'after', 'fifty', 'together', 'already', 'some', 'when', 'hence', 'never', 'this', 'system', 'throughout', 'two', 'both', 'onto', 'con', 'while', 'describe', 'go', 'noone', 'twelve', 'from', 'was', 'therein', 'because', 'among', 'etc', 'anywhere', 'yourself', 'may', 'for', 'bottom', 'former', 'per', 'became', 'indeed', 'much', 'beyond', 'yet', 'behind', 'eg', 'detail', 'thereby', 'least', 'most', 'ourselves', 'until', 'were', 'with', 'even', 'nobody', 'ltd', 'further', 'whole', 'his', 'amount', 'however', 'sometime', 'being', 'show', 'at', 'often', 'done', 'becomes', 'above', 'towards', 'itself', 'hereupon', 'either', 'of', 'something', 'and', 'have', 'no', 'whereupon', 'elsewhere', 'becoming', 'where', 'whenever', 'sincere', 'can', 'be', 'on', 'mine', 'yours', 'thereupon', 'but', 'hers', 'full', 'enough', 'otherwise', 'found', 'them', 'amoungst', 'these', 'yourselves', 'must', 'below', 'not', 'am', 'fire', 'it', 'he', 'perhaps', 'toward', 'thence', 'more', 'very', 'un', 'due', 'been', 'such', 'now', 'who', 'its', 'so', 'namely', 'sometimes', 'thereafter', 're', 'eight', 'me', 'other', 'those', 'are', 'nothing', 'whose', 'give', 'the', 'inc', 'seemed', 'someone', 'put', 'i', 'alone', 'latter', 'what', 'many', 'out', 'thin', 'whom', 'except', 'herself', 'will', 'first', 'beside', 'she', 'therefore', 'else', 'her', 'cant', 'during', 'few', 'via', 'why', 'wherein', 'is', 'as', 'last', 'though', 'moreover', 'whether', 'call', 'others', 'mostly', 'empty', 'part', 'hereby', 'own', 'wherever', 'nine', 'five', 'their', 'top', 'nor', 'before', 'into', 'back', 'name', 'almost', 'too', 'front', 'then', 'six', 'neither', 'to', 'anyway', 'there', 'has', 'thus'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'there', 'becomes', 'became', 'top', 'without', 'onto', 'namely', 'hers', 'more', 'behind', 'whose', 'bottom', 'have', 'into', 'his', 'are', 'than', 'that', 'who', 'fire', 'such', 'empty', 'how', 'five', 'per', 'beside', 'thick', 'name', 'should', 'first', 'after', 'whither', 'therein', 'had', 'few', 'nothing', 'every', 'others', 'fifty', 'yourselves', 'must', 'your', 'no', 'done', 'forty', 'now', 'her', 'everywhere', 'i', 'on', 'toward', 'thereupon', 'several', 'ten', 'my', 'same', 'almost', 'hereby', 'back', 'inc', 'show', 'throughout', 'yet', 'is', 'within', 'mill', 'describe', 'front', 'both', 'keep', 'four', 'these', 'next', 'might', 'together', 'herself', 'whence', 'besides', 'above', 'nobody', 'take', 'found', 'mostly', 'everyone', 'were', 'otherwise', 'co', 'please', 'except', 'three', 'however', 'via', 'upon', 'our', 'it', 'enough', 'to', 'yourself', 'wherein', 'during', 'we', 'seeming', 'therefore', 'find', 'many', 'because', 'hasnt', 'ourselves', 'whoever', 'wherever', 'six', 'own', 'move', 'may', 'thru', 'would', 'something', 'further', 'yours', 'whereby', 'see', 'but', 'amoungst', 'where', 'most', 'under', 'get', 'sometimes', 'hereafter', 'con', 'least', 'anywhere', 'has', 'un', 'ltd', 'them', 'noone', 'then', 'us', 'meanwhile', 'whatever', 'anyhow', 'well', 'while', 'so', 'themselves', 'down', 'you', 'twelve', 'some', 'too', 'rather', 'moreover', 'if', 'seem', 'afterwards', 'along', 'eight', 'seemed', 'any', 'cant', 'between', 'this', 'do', 'whereafter', 'already', 'whom', 'once', 'beyond', 'beforehand', 'myself', 'the', 'formerly', 'thence', 'sincere', 'though', 'am', 'elsewhere', 'from', 'ever', 'perhaps', 'call', 'again', 'fifteen', 'what', 'sometime', 'thereby', 'interest', 'fill', 're', 'former', 'him', 'they', 'less', 'not', 'everything', 'nevertheless', 'across', 'always', 'up', 'or', 'off', 'before', 'by', 'made', 'alone', 'thin', 'system', 'other', 'me', 'give', 'cannot', 'due', 'below', 'eg', 'neither', 'of', 'can', 'for', 'also', 'twenty', 'an', 'among', 'even', 'much', 'being', 'could', 'hence', 'with', 'himself', 'someone', 'thus', 'amongst', 'when', 'through', 'one', 'couldnt', 'each', 'against', 'becoming', 'in', 'whenever', 'will', 'anyone', 'detail', 'latterly', 'somehow', 'thereafter', 'never', 'de', 'nor', 'only', 'full', 'here', 'was', 'put', 'still', 'somewhere', 'and', 'he', 'often', 'go', 'serious', 'seems', 'she', 'itself', 'last', 'their', 'all', 'cry', 'since', 'part', 'ie', 'nowhere', 'whether', 'as', 'another', 'at', 'towards', 'hereupon', 'whole', 'whereupon', 'side', 'those', 'out', 'latter', 'hundred', 'etc', 'anyway', 'indeed', 'bill', 'been', 'amount', 'its', 'be', 'which', 'third', 'become', 'why', 'nine', 'although', 'over', 'about', 'herein', 'around', 'anything', 'eleven', 'very', 'sixty', 'none', 'two', 'until', 'ours', 'mine', 'either', 'whereas', 'a', 'else'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11f272a20>; total time= 2.0min\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'same', 'or', 'see', 'whatever', 'a', 'they', 'than', 'hereafter', 'meanwhile', 'if', 'over', 'whence', 'again', 'well', 'cannot', 'bill', 'ours', 'whither', 'everywhere', 'along', 'move', 'ever', 'off', 'twenty', 'interest', 'against', 'one', 'keep', 'any', 'sixty', 'three', 'cry', 'all', 'could', 'anyone', 'also', 'anyhow', 'although', 'him', 'next', 'upon', 'none', 'up', 'serious', 'another', 'my', 'de', 'themselves', 'how', 'everything', 'thru', 'fill', 'your', 'myself', 'forty', 'an', 'himself', 'four', 'whoever', 'should', 'down', 'side', 'everyone', 'our', 'nowhere', 'seeming', 'third', 'eleven', 'by', 'since', 'whereas', 'within', 'seem', 'us', 'take', 'nevertheless', 'somehow', 'formerly', 'might', 'less', 'latterly', 'which', 'every', 'would', 'still', 'between', 'about', 'across', 'had', 'thick', 'through', 'fifteen', 'whereby', 'get', 'anything', 'find', 'whereafter', 'become', 'herein', 'several', 'seems', 'we', 'around', 'somewhere', 'mill', 'ten', 'that', 'besides', 'beforehand', 'co', 'hasnt', 'please', 'ie', 'without', 'amongst', 'once', 'made', 'you', 'under', 'in', 'rather', 'couldnt', 'only', 'hundred', 'do', 'always', 'here', 'afterwards', 'each', 'after', 'fifty', 'together', 'already', 'some', 'when', 'hence', 'never', 'this', 'system', 'throughout', 'two', 'both', 'onto', 'con', 'while', 'describe', 'go', 'noone', 'twelve', 'from', 'was', 'therein', 'because', 'among', 'etc', 'anywhere', 'yourself', 'may', 'for', 'bottom', 'former', 'per', 'became', 'indeed', 'much', 'beyond', 'yet', 'behind', 'eg', 'detail', 'thereby', 'least', 'most', 'ourselves', 'until', 'were', 'with', 'even', 'nobody', 'ltd', 'further', 'whole', 'his', 'amount', 'however', 'sometime', 'being', 'show', 'at', 'often', 'done', 'becomes', 'above', 'towards', 'itself', 'hereupon', 'either', 'of', 'something', 'and', 'have', 'no', 'whereupon', 'elsewhere', 'becoming', 'where', 'whenever', 'sincere', 'can', 'be', 'on', 'mine', 'yours', 'thereupon', 'but', 'hers', 'full', 'enough', 'otherwise', 'found', 'them', 'amoungst', 'these', 'yourselves', 'must', 'below', 'not', 'am', 'fire', 'it', 'he', 'perhaps', 'toward', 'thence', 'more', 'very', 'un', 'due', 'been', 'such', 'now', 'who', 'its', 'so', 'namely', 'sometimes', 'thereafter', 're', 'eight', 'me', 'other', 'those', 'are', 'nothing', 'whose', 'give', 'the', 'inc', 'seemed', 'someone', 'put', 'i', 'alone', 'latter', 'what', 'many', 'out', 'thin', 'whom', 'except', 'herself', 'will', 'first', 'beside', 'she', 'therefore', 'else', 'her', 'cant', 'during', 'few', 'via', 'why', 'wherein', 'is', 'as', 'last', 'though', 'moreover', 'whether', 'call', 'others', 'mostly', 'empty', 'part', 'hereby', 'own', 'wherever', 'nine', 'five', 'their', 'top', 'nor', 'before', 'into', 'back', 'name', 'almost', 'too', 'front', 'then', 'six', 'neither', 'to', 'anyway', 'there', 'has', 'thus'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'there', 'becomes', 'became', 'top', 'without', 'onto', 'namely', 'hers', 'more', 'behind', 'whose', 'bottom', 'have', 'into', 'his', 'are', 'than', 'that', 'who', 'fire', 'such', 'empty', 'how', 'five', 'per', 'beside', 'thick', 'name', 'should', 'first', 'after', 'whither', 'therein', 'had', 'few', 'nothing', 'every', 'others', 'fifty', 'yourselves', 'must', 'your', 'no', 'done', 'forty', 'now', 'her', 'everywhere', 'i', 'on', 'toward', 'thereupon', 'several', 'ten', 'my', 'same', 'almost', 'hereby', 'back', 'inc', 'show', 'throughout', 'yet', 'is', 'within', 'mill', 'describe', 'front', 'both', 'keep', 'four', 'these', 'next', 'might', 'together', 'herself', 'whence', 'besides', 'above', 'nobody', 'take', 'found', 'mostly', 'everyone', 'were', 'otherwise', 'co', 'please', 'except', 'three', 'however', 'via', 'upon', 'our', 'it', 'enough', 'to', 'yourself', 'wherein', 'during', 'we', 'seeming', 'therefore', 'find', 'many', 'because', 'hasnt', 'ourselves', 'whoever', 'wherever', 'six', 'own', 'move', 'may', 'thru', 'would', 'something', 'further', 'yours', 'whereby', 'see', 'but', 'amoungst', 'where', 'most', 'under', 'get', 'sometimes', 'hereafter', 'con', 'least', 'anywhere', 'has', 'un', 'ltd', 'them', 'noone', 'then', 'us', 'meanwhile', 'whatever', 'anyhow', 'well', 'while', 'so', 'themselves', 'down', 'you', 'twelve', 'some', 'too', 'rather', 'moreover', 'if', 'seem', 'afterwards', 'along', 'eight', 'seemed', 'any', 'cant', 'between', 'this', 'do', 'whereafter', 'already', 'whom', 'once', 'beyond', 'beforehand', 'myself', 'the', 'formerly', 'thence', 'sincere', 'though', 'am', 'elsewhere', 'from', 'ever', 'perhaps', 'call', 'again', 'fifteen', 'what', 'sometime', 'thereby', 'interest', 'fill', 're', 'former', 'him', 'they', 'less', 'not', 'everything', 'nevertheless', 'across', 'always', 'up', 'or', 'off', 'before', 'by', 'made', 'alone', 'thin', 'system', 'other', 'me', 'give', 'cannot', 'due', 'below', 'eg', 'neither', 'of', 'can', 'for', 'also', 'twenty', 'an', 'among', 'even', 'much', 'being', 'could', 'hence', 'with', 'himself', 'someone', 'thus', 'amongst', 'when', 'through', 'one', 'couldnt', 'each', 'against', 'becoming', 'in', 'whenever', 'will', 'anyone', 'detail', 'latterly', 'somehow', 'thereafter', 'never', 'de', 'nor', 'only', 'full', 'here', 'was', 'put', 'still', 'somewhere', 'and', 'he', 'often', 'go', 'serious', 'seems', 'she', 'itself', 'last', 'their', 'all', 'cry', 'since', 'part', 'ie', 'nowhere', 'whether', 'as', 'another', 'at', 'towards', 'hereupon', 'whole', 'whereupon', 'side', 'those', 'out', 'latter', 'hundred', 'etc', 'anyway', 'indeed', 'bill', 'been', 'amount', 'its', 'be', 'which', 'third', 'become', 'why', 'nine', 'although', 'over', 'about', 'herein', 'around', 'anything', 'eleven', 'very', 'sixty', 'none', 'two', 'until', 'ours', 'mine', 'either', 'whereas', 'a', 'else'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x1257267a0>; total time= 2.0min\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'keep', 'whither', 'however', 'whereby', 'or', 'off', 'such', 'since', 'find', 'being', 'might', 'most', 'must', 'sincere', 'formerly', 're', 'many', 'something', 'rather', 'more', 'nothing', 'side', 'namely', 'our', 'each', 'over', 'these', 'thereafter', 'first', 'any', 'enough', 'nowhere', 'become', 'under', 'two', 'five', 'thence', 'everyone', 'whereafter', 'bottom', 'so', 'beside', 'with', 'none', 'where', 'should', 'all', 'been', 'full', 'yours', 'across', 'here', 'whenever', 'hers', 'few', 'couldnt', 'myself', 'detail', 'anyhow', 'last', 'even', 'six', 'further', 'afterwards', 'cant', 'moreover', 'we', 'can', 'while', 'much', 'forty', 'else', 'eg', 'please', 'next', 'would', 'anyone', 'whatever', 'in', 'towards', 'the', 'fire', 'yourselves', 'everything', 'put', 'by', 'it', 'then', 'therefore', 'from', 'empty', 'whether', 'either', 'nobody', 'almost', 'those', 'becomes', 'ten', 'nevertheless', 'ours', 'con', 'their', 'somewhere', 'whereas', 'is', 'whoever', 'thereupon', 'indeed', 'without', 'us', 'up', 'un', 'as', 'hasnt', 'what', 'least', 'per', 'her', 'until', 'hereafter', 'everywhere', 'sometimes', 'inc', 'together', 'several', 'cannot', 'him', 'seemed', 'front', 'serious', 'done', 'de', 'hence', 'to', 'only', 'have', 'thru', 'part', 'onto', 'could', 'amongst', 'of', 'another', 'became', 'hundred', 'how', 'within', 'three', 'behind', 'third', 'too', 'wherein', 'anyway', 'through', 'although', 'becoming', 'herself', 'former', 'will', 'along', 'latterly', 'hereby', 'when', 'they', 'cry', 'whence', 'anything', 'herein', 'below', 'well', 'your', 'interest', 'noone', 'also', 'twelve', 'still', 'found', 'ourselves', 'me', 'once', 'name', 'upon', 'show', 'other', 'about', 'nine', 'perhaps', 'its', 'alone', 'she', 'above', 'bill', 'very', 'etc', 'some', 'seem', 'his', 'than', 'get', 'fifteen', 'thus', 'that', 'himself', 'are', 'eleven', 'amount', 'whom', 'wherever', 'nor', 'hereupon', 'thin', 'yourself', 'ltd', 'twenty', 'though', 'i', 'do', 'somehow', 'sixty', 'four', 'made', 'beforehand', 'fill', 'therein', 'yet', 'thereby', 'otherwise', 'mill', 'amoungst', 'neither', 'if', 'an', 'a', 'were', 'on', 'beyond', 'less', 'against', 'after', 'there', 'now', 'mine', 'top', 'out', 'mostly', 'between', 'both', 'due', 'same', 'which', 'at', 'because', 'no', 'system', 'was', 'throughout', 'who', 'itself', 'themselves', 'whole', 'go', 'down', 'describe', 'sometime', 'before', 'others', 'one', 'always', 'he', 'but', 'give', 'during', 'may', 'them', 'call', 'anywhere', 'back', 'eight', 'again', 'via', 'am', 'ie', 'often', 'has', 'someone', 'ever', 'you', 'among', 'for', 'take', 'be', 'and', 'seems', 'co', 'seeming', 'why', 'besides', 'elsewhere', 'around', 'whose', 'see', 'not', 'toward', 'already', 'own', 'thick', 'whereupon', 'never', 'had', 'every', 'move', 'my', 'except', 'into', 'meanwhile', 'fifty', 'this', 'latter'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'con', 'beforehand', 'show', 'for', 'over', 'empty', 'more', 'thin', 'four', 'towards', 'herein', 'hence', 'until', 'among', 'will', 'become', 'former', 'next', 'thick', 'whole', 'whereas', 'never', 'hereafter', 'everything', 'is', 'our', 'during', 'while', 'than', 'their', 'it', 'others', 'that', 'down', 'latterly', 'fill', 'herself', 'whenever', 'his', 'ltd', 'eg', 'several', 'side', 'since', 'my', 'your', 'see', 'up', 'when', 'are', 'becomes', 'every', 'were', 'everywhere', 'would', 'indeed', 'yet', 'interest', 'ten', 'namely', 'elsewhere', 'anything', 'couldnt', 'seemed', 'of', 'name', 'you', 'becoming', 'move', 'very', 'third', 'nine', 'whom', 'around', 'sometime', 'beyond', 'wherever', 'twenty', 'seems', 'too', 'below', 'forty', 'between', 'who', 'most', 'thence', 'behind', 'etc', 'otherwise', 'once', 'have', 'those', 'above', 'be', 'we', 'only', 'to', 'alone', 'off', 'full', 'nobody', 'hasnt', 'eight', 'he', 'hundred', 'un', 'fifteen', 'became', 'before', 'first', 'one', 'seem', 'the', 'ie', 'am', 'whose', 'some', 're', 'whence', 'thereupon', 'now', 'can', 'meanwhile', 'call', 'whereby', 'itself', 'however', 'being', 'then', 'enough', 'last', 'as', 'within', 'on', 'none', 'still', 'almost', 'co', 'thus', 'across', 'many', 'besides', 'moreover', 'must', 'under', 'also', 'anywhere', 'hereby', 'hers', 'get', 'bill', 'twelve', 'even', 'amoungst', 'which', 'was', 'do', 'well', 'de', 'they', 'himself', 'these', 'perhaps', 'serious', 'her', 'us', 'its', 'without', 'if', 'might', 'how', 'back', 'someone', 'why', 'ourselves', 'whoever', 'fifty', 'whereafter', 'other', 'neither', 'themselves', 'but', 'via', 'an', 'or', 'each', 'yours', 'hereupon', 'six', 'nothing', 'any', 'either', 'not', 'nevertheless', 'where', 'rather', 'put', 'detail', 'may', 'along', 'nowhere', 'made', 'should', 'anyway', 'has', 'sixty', 'system', 'done', 'anyhow', 'what', 'cry', 'latter', 'both', 'three', 'she', 'found', 'in', 'often', 'therein', 'beside', 'all', 'therefore', 'much', 'sometimes', 'out', 'top', 'per', 'by', 'noone', 'give', 'part', 'least', 'ours', 'here', 'cant', 'and', 'yourselves', 'else', 'so', 'yourself', 'into', 'due', 'mill', 'with', 'a', 'me', 'upon', 'after', 'together', 'anyone', 'always', 'mine', 'thru', 'again', 'had', 'myself', 'thereby', 'against', 'although', 'two', 'whether', 'amongst', 'been', 'bottom', 'afterwards', 'own', 'ever', 'throughout', 'nor', 'whereupon', 'another', 'find', 'except', 'this', 'please', 'somewhere', 'seeming', 'five', 'go', 'toward', 'at', 'take', 'because', 'fire', 'thereafter', 'mostly', 'somehow', 'no', 'keep', 'amount', 'them', 'whatever', 'everyone', 'i', 'eleven', 'whither', 'though', 'formerly', 'wherein', 'describe', 'same', 'onto', 'through', 'further', 'from', 'sincere', 'less', 'there', 'him', 'inc', 'about', 'already', 'such', 'something', 'could', 'cannot', 'few', 'front'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'there', 'becomes', 'became', 'top', 'without', 'onto', 'namely', 'hers', 'more', 'behind', 'whose', 'bottom', 'have', 'into', 'his', 'are', 'than', 'that', 'who', 'fire', 'such', 'empty', 'how', 'five', 'per', 'beside', 'thick', 'name', 'should', 'first', 'after', 'whither', 'therein', 'had', 'few', 'nothing', 'every', 'others', 'fifty', 'yourselves', 'must', 'your', 'no', 'done', 'forty', 'now', 'her', 'everywhere', 'i', 'on', 'toward', 'thereupon', 'several', 'ten', 'my', 'same', 'almost', 'hereby', 'back', 'inc', 'show', 'throughout', 'yet', 'is', 'within', 'mill', 'describe', 'front', 'both', 'keep', 'four', 'these', 'next', 'might', 'together', 'herself', 'whence', 'besides', 'above', 'nobody', 'take', 'found', 'mostly', 'everyone', 'were', 'otherwise', 'co', 'please', 'except', 'three', 'however', 'via', 'upon', 'our', 'it', 'enough', 'to', 'yourself', 'wherein', 'during', 'we', 'seeming', 'therefore', 'find', 'many', 'because', 'hasnt', 'ourselves', 'whoever', 'wherever', 'six', 'own', 'move', 'may', 'thru', 'would', 'something', 'further', 'yours', 'whereby', 'see', 'but', 'amoungst', 'where', 'most', 'under', 'get', 'sometimes', 'hereafter', 'con', 'least', 'anywhere', 'has', 'un', 'ltd', 'them', 'noone', 'then', 'us', 'meanwhile', 'whatever', 'anyhow', 'well', 'while', 'so', 'themselves', 'down', 'you', 'twelve', 'some', 'too', 'rather', 'moreover', 'if', 'seem', 'afterwards', 'along', 'eight', 'seemed', 'any', 'cant', 'between', 'this', 'do', 'whereafter', 'already', 'whom', 'once', 'beyond', 'beforehand', 'myself', 'the', 'formerly', 'thence', 'sincere', 'though', 'am', 'elsewhere', 'from', 'ever', 'perhaps', 'call', 'again', 'fifteen', 'what', 'sometime', 'thereby', 'interest', 'fill', 're', 'former', 'him', 'they', 'less', 'not', 'everything', 'nevertheless', 'across', 'always', 'up', 'or', 'off', 'before', 'by', 'made', 'alone', 'thin', 'system', 'other', 'me', 'give', 'cannot', 'due', 'below', 'eg', 'neither', 'of', 'can', 'for', 'also', 'twenty', 'an', 'among', 'even', 'much', 'being', 'could', 'hence', 'with', 'himself', 'someone', 'thus', 'amongst', 'when', 'through', 'one', 'couldnt', 'each', 'against', 'becoming', 'in', 'whenever', 'will', 'anyone', 'detail', 'latterly', 'somehow', 'thereafter', 'never', 'de', 'nor', 'only', 'full', 'here', 'was', 'put', 'still', 'somewhere', 'and', 'he', 'often', 'go', 'serious', 'seems', 'she', 'itself', 'last', 'their', 'all', 'cry', 'since', 'part', 'ie', 'nowhere', 'whether', 'as', 'another', 'at', 'towards', 'hereupon', 'whole', 'whereupon', 'side', 'those', 'out', 'latter', 'hundred', 'etc', 'anyway', 'indeed', 'bill', 'been', 'amount', 'its', 'be', 'which', 'third', 'become', 'why', 'nine', 'although', 'over', 'about', 'herein', 'around', 'anything', 'eleven', 'very', 'sixty', 'none', 'two', 'until', 'ours', 'mine', 'either', 'whereas', 'a', 'else'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'con', 'beforehand', 'show', 'for', 'over', 'empty', 'more', 'thin', 'four', 'towards', 'herein', 'hence', 'until', 'among', 'will', 'become', 'former', 'next', 'thick', 'whole', 'whereas', 'never', 'hereafter', 'everything', 'is', 'our', 'during', 'while', 'than', 'their', 'it', 'others', 'that', 'down', 'latterly', 'fill', 'herself', 'whenever', 'his', 'ltd', 'eg', 'several', 'side', 'since', 'my', 'your', 'see', 'up', 'when', 'are', 'becomes', 'every', 'were', 'everywhere', 'would', 'indeed', 'yet', 'interest', 'ten', 'namely', 'elsewhere', 'anything', 'couldnt', 'seemed', 'of', 'name', 'you', 'becoming', 'move', 'very', 'third', 'nine', 'whom', 'around', 'sometime', 'beyond', 'wherever', 'twenty', 'seems', 'too', 'below', 'forty', 'between', 'who', 'most', 'thence', 'behind', 'etc', 'otherwise', 'once', 'have', 'those', 'above', 'be', 'we', 'only', 'to', 'alone', 'off', 'full', 'nobody', 'hasnt', 'eight', 'he', 'hundred', 'un', 'fifteen', 'became', 'before', 'first', 'one', 'seem', 'the', 'ie', 'am', 'whose', 'some', 're', 'whence', 'thereupon', 'now', 'can', 'meanwhile', 'call', 'whereby', 'itself', 'however', 'being', 'then', 'enough', 'last', 'as', 'within', 'on', 'none', 'still', 'almost', 'co', 'thus', 'across', 'many', 'besides', 'moreover', 'must', 'under', 'also', 'anywhere', 'hereby', 'hers', 'get', 'bill', 'twelve', 'even', 'amoungst', 'which', 'was', 'do', 'well', 'de', 'they', 'himself', 'these', 'perhaps', 'serious', 'her', 'us', 'its', 'without', 'if', 'might', 'how', 'back', 'someone', 'why', 'ourselves', 'whoever', 'fifty', 'whereafter', 'other', 'neither', 'themselves', 'but', 'via', 'an', 'or', 'each', 'yours', 'hereupon', 'six', 'nothing', 'any', 'either', 'not', 'nevertheless', 'where', 'rather', 'put', 'detail', 'may', 'along', 'nowhere', 'made', 'should', 'anyway', 'has', 'sixty', 'system', 'done', 'anyhow', 'what', 'cry', 'latter', 'both', 'three', 'she', 'found', 'in', 'often', 'therein', 'beside', 'all', 'therefore', 'much', 'sometimes', 'out', 'top', 'per', 'by', 'noone', 'give', 'part', 'least', 'ours', 'here', 'cant', 'and', 'yourselves', 'else', 'so', 'yourself', 'into', 'due', 'mill', 'with', 'a', 'me', 'upon', 'after', 'together', 'anyone', 'always', 'mine', 'thru', 'again', 'had', 'myself', 'thereby', 'against', 'although', 'two', 'whether', 'amongst', 'been', 'bottom', 'afterwards', 'own', 'ever', 'throughout', 'nor', 'whereupon', 'another', 'find', 'except', 'this', 'please', 'somewhere', 'seeming', 'five', 'go', 'toward', 'at', 'take', 'because', 'fire', 'thereafter', 'mostly', 'somehow', 'no', 'keep', 'amount', 'them', 'whatever', 'everyone', 'i', 'eleven', 'whither', 'though', 'formerly', 'wherein', 'describe', 'same', 'onto', 'through', 'further', 'from', 'sincere', 'less', 'there', 'him', 'inc', 'about', 'already', 'such', 'something', 'could', 'cannot', 'few', 'front'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'keep', 'whither', 'however', 'whereby', 'or', 'off', 'such', 'since', 'find', 'being', 'might', 'most', 'must', 'sincere', 'formerly', 're', 'many', 'something', 'rather', 'more', 'nothing', 'side', 'namely', 'our', 'each', 'over', 'these', 'thereafter', 'first', 'any', 'enough', 'nowhere', 'become', 'under', 'two', 'five', 'thence', 'everyone', 'whereafter', 'bottom', 'so', 'beside', 'with', 'none', 'where', 'should', 'all', 'been', 'full', 'yours', 'across', 'here', 'whenever', 'hers', 'few', 'couldnt', 'myself', 'detail', 'anyhow', 'last', 'even', 'six', 'further', 'afterwards', 'cant', 'moreover', 'we', 'can', 'while', 'much', 'forty', 'else', 'eg', 'please', 'next', 'would', 'anyone', 'whatever', 'in', 'towards', 'the', 'fire', 'yourselves', 'everything', 'put', 'by', 'it', 'then', 'therefore', 'from', 'empty', 'whether', 'either', 'nobody', 'almost', 'those', 'becomes', 'ten', 'nevertheless', 'ours', 'con', 'their', 'somewhere', 'whereas', 'is', 'whoever', 'thereupon', 'indeed', 'without', 'us', 'up', 'un', 'as', 'hasnt', 'what', 'least', 'per', 'her', 'until', 'hereafter', 'everywhere', 'sometimes', 'inc', 'together', 'several', 'cannot', 'him', 'seemed', 'front', 'serious', 'done', 'de', 'hence', 'to', 'only', 'have', 'thru', 'part', 'onto', 'could', 'amongst', 'of', 'another', 'became', 'hundred', 'how', 'within', 'three', 'behind', 'third', 'too', 'wherein', 'anyway', 'through', 'although', 'becoming', 'herself', 'former', 'will', 'along', 'latterly', 'hereby', 'when', 'they', 'cry', 'whence', 'anything', 'herein', 'below', 'well', 'your', 'interest', 'noone', 'also', 'twelve', 'still', 'found', 'ourselves', 'me', 'once', 'name', 'upon', 'show', 'other', 'about', 'nine', 'perhaps', 'its', 'alone', 'she', 'above', 'bill', 'very', 'etc', 'some', 'seem', 'his', 'than', 'get', 'fifteen', 'thus', 'that', 'himself', 'are', 'eleven', 'amount', 'whom', 'wherever', 'nor', 'hereupon', 'thin', 'yourself', 'ltd', 'twenty', 'though', 'i', 'do', 'somehow', 'sixty', 'four', 'made', 'beforehand', 'fill', 'therein', 'yet', 'thereby', 'otherwise', 'mill', 'amoungst', 'neither', 'if', 'an', 'a', 'were', 'on', 'beyond', 'less', 'against', 'after', 'there', 'now', 'mine', 'top', 'out', 'mostly', 'between', 'both', 'due', 'same', 'which', 'at', 'because', 'no', 'system', 'was', 'throughout', 'who', 'itself', 'themselves', 'whole', 'go', 'down', 'describe', 'sometime', 'before', 'others', 'one', 'always', 'he', 'but', 'give', 'during', 'may', 'them', 'call', 'anywhere', 'back', 'eight', 'again', 'via', 'am', 'ie', 'often', 'has', 'someone', 'ever', 'you', 'among', 'for', 'take', 'be', 'and', 'seems', 'co', 'seeming', 'why', 'besides', 'elsewhere', 'around', 'whose', 'see', 'not', 'toward', 'already', 'own', 'thick', 'whereupon', 'never', 'had', 'every', 'move', 'my', 'except', 'into', 'meanwhile', 'fifty', 'this', 'latter'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'con', 'beforehand', 'show', 'for', 'over', 'empty', 'more', 'thin', 'four', 'towards', 'herein', 'hence', 'until', 'among', 'will', 'become', 'former', 'next', 'thick', 'whole', 'whereas', 'never', 'hereafter', 'everything', 'is', 'our', 'during', 'while', 'than', 'their', 'it', 'others', 'that', 'down', 'latterly', 'fill', 'herself', 'whenever', 'his', 'ltd', 'eg', 'several', 'side', 'since', 'my', 'your', 'see', 'up', 'when', 'are', 'becomes', 'every', 'were', 'everywhere', 'would', 'indeed', 'yet', 'interest', 'ten', 'namely', 'elsewhere', 'anything', 'couldnt', 'seemed', 'of', 'name', 'you', 'becoming', 'move', 'very', 'third', 'nine', 'whom', 'around', 'sometime', 'beyond', 'wherever', 'twenty', 'seems', 'too', 'below', 'forty', 'between', 'who', 'most', 'thence', 'behind', 'etc', 'otherwise', 'once', 'have', 'those', 'above', 'be', 'we', 'only', 'to', 'alone', 'off', 'full', 'nobody', 'hasnt', 'eight', 'he', 'hundred', 'un', 'fifteen', 'became', 'before', 'first', 'one', 'seem', 'the', 'ie', 'am', 'whose', 'some', 're', 'whence', 'thereupon', 'now', 'can', 'meanwhile', 'call', 'whereby', 'itself', 'however', 'being', 'then', 'enough', 'last', 'as', 'within', 'on', 'none', 'still', 'almost', 'co', 'thus', 'across', 'many', 'besides', 'moreover', 'must', 'under', 'also', 'anywhere', 'hereby', 'hers', 'get', 'bill', 'twelve', 'even', 'amoungst', 'which', 'was', 'do', 'well', 'de', 'they', 'himself', 'these', 'perhaps', 'serious', 'her', 'us', 'its', 'without', 'if', 'might', 'how', 'back', 'someone', 'why', 'ourselves', 'whoever', 'fifty', 'whereafter', 'other', 'neither', 'themselves', 'but', 'via', 'an', 'or', 'each', 'yours', 'hereupon', 'six', 'nothing', 'any', 'either', 'not', 'nevertheless', 'where', 'rather', 'put', 'detail', 'may', 'along', 'nowhere', 'made', 'should', 'anyway', 'has', 'sixty', 'system', 'done', 'anyhow', 'what', 'cry', 'latter', 'both', 'three', 'she', 'found', 'in', 'often', 'therein', 'beside', 'all', 'therefore', 'much', 'sometimes', 'out', 'top', 'per', 'by', 'noone', 'give', 'part', 'least', 'ours', 'here', 'cant', 'and', 'yourselves', 'else', 'so', 'yourself', 'into', 'due', 'mill', 'with', 'a', 'me', 'upon', 'after', 'together', 'anyone', 'always', 'mine', 'thru', 'again', 'had', 'myself', 'thereby', 'against', 'although', 'two', 'whether', 'amongst', 'been', 'bottom', 'afterwards', 'own', 'ever', 'throughout', 'nor', 'whereupon', 'another', 'find', 'except', 'this', 'please', 'somewhere', 'seeming', 'five', 'go', 'toward', 'at', 'take', 'because', 'fire', 'thereafter', 'mostly', 'somehow', 'no', 'keep', 'amount', 'them', 'whatever', 'everyone', 'i', 'eleven', 'whither', 'though', 'formerly', 'wherein', 'describe', 'same', 'onto', 'through', 'further', 'from', 'sincere', 'less', 'there', 'him', 'inc', 'about', 'already', 'such', 'something', 'could', 'cannot', 'few', 'front'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'ever', 'thence', 'others', 'formerly', 'call', 'somewhere', 'twenty', 'might', 'last', 'hence', 'wherever', 'about', 'anywhere', 'everywhere', 'can', 'neither', 'towards', 'front', 'cannot', 'both', 'once', 'himself', 'five', 'latter', 'via', 'another', 'our', 'amongst', 'which', 'above', 'us', 'while', 'part', 'whether', 'every', 'six', 'yourself', 'been', 'under', 'such', 'together', 'else', 'hundred', 'made', 'afterwards', 'whatever', 'give', 'go', 'three', 'however', 'for', 'though', 'none', 'may', 'themselves', 'other', 'seemed', 'top', 'now', 'thereby', 'or', 'sincere', 'are', 'any', 'describe', 'they', 'then', 'everything', 'further', 'amount', 'two', 'whole', 'it', 'whereupon', 'keep', 'per', 'nevertheless', 'but', 'of', 'she', 'see', 'several', 'myself', 'cry', 'many', 'hereafter', 'although', 'bill', 'sixty', 'latterly', 'done', 'former', 'perhaps', 'against', 'due', 'side', 'hasnt', 'even', 'who', 'than', 'around', 'with', 'name', 'across', 'him', 'is', 'by', 'always', 'whoever', 'his', 'much', 'all', 'anyway', 'toward', 'fifteen', 'anyhow', 'has', 'would', 'seeming', 'anyone', 'therein', 'full', 'please', 'am', 'beforehand', 'serious', 're', 'nine', 'same', 'enough', 'how', 'often', 'inc', 'again', 'do', 'because', 'anything', 'alone', 'get', 'them', 'between', 'ourselves', 'become', 'had', 'move', 'those', 'very', 'and', 'i', 'system', 'thin', 'seems', 'up', 'in', 'a', 'ie', 'so', 'next', 'beside', 'until', 'something', 'moreover', 'first', 'without', 'off', 'among', 'nobody', 'found', 'whereafter', 'becoming', 'few', 'mine', 'you', 'me', 'twelve', 'interest', 'he', 'beyond', 'also', 'take', 'behind', 'these', 'least', 'well', 'four', 'meanwhile', 'someone', 'everyone', 'etc', 'still', 'thru', 'this', 'most', 'sometime', 'find', 'detail', 'therefore', 'whereby', 'except', 'eight', 'be', 'nothing', 'to', 'before', 'thereafter', 'whenever', 'bottom', 'hereby', 'within', 'co', 'wherein', 'eg', 'being', 'their', 'itself', 'hereupon', 'what', 'ours', 'thick', 'mill', 'each', 'noone', 'one', 'should', 'when', 'no', 'forty', 'namely', 'will', 'the', 'here', 'onto', 'besides', 'your', 'on', 'whereas', 'whose', 'de', 'why', 'fill', 'if', 'herein', 'yet', 'con', 'seem', 'more', 'somehow', 'rather', 'must', 'elsewhere', 'an', 'too', 'into', 'was', 'were', 'show', 'we', 'after', 'cant', 'since', 'through', 'never', 'not', 'fire', 'during', 'ten', 'yours', 'could', 'empty', 'from', 'put', 'thus', 'whom', 'became', 'whither', 'thereupon', 'less', 'un', 'indeed', 'mostly', 'throughout', 'out', 'own', 'nor', 'hers', 'where', 'that', 'back', 'some', 'only', 'my', 'sometimes', 'third', 'upon', 'herself', 'already', 'fifty', 'as', 'its', 'nowhere', 'ltd', 'yourselves', 'otherwise', 'almost', 'at', 'there', 'down', 'becomes', 'couldnt', 'either', 'amoungst', 'over', 'eleven', 'have', 'along', 'below', 'her', 'whence'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'keep', 'whither', 'however', 'whereby', 'or', 'off', 'such', 'since', 'find', 'being', 'might', 'most', 'must', 'sincere', 'formerly', 're', 'many', 'something', 'rather', 'more', 'nothing', 'side', 'namely', 'our', 'each', 'over', 'these', 'thereafter', 'first', 'any', 'enough', 'nowhere', 'become', 'under', 'two', 'five', 'thence', 'everyone', 'whereafter', 'bottom', 'so', 'beside', 'with', 'none', 'where', 'should', 'all', 'been', 'full', 'yours', 'across', 'here', 'whenever', 'hers', 'few', 'couldnt', 'myself', 'detail', 'anyhow', 'last', 'even', 'six', 'further', 'afterwards', 'cant', 'moreover', 'we', 'can', 'while', 'much', 'forty', 'else', 'eg', 'please', 'next', 'would', 'anyone', 'whatever', 'in', 'towards', 'the', 'fire', 'yourselves', 'everything', 'put', 'by', 'it', 'then', 'therefore', 'from', 'empty', 'whether', 'either', 'nobody', 'almost', 'those', 'becomes', 'ten', 'nevertheless', 'ours', 'con', 'their', 'somewhere', 'whereas', 'is', 'whoever', 'thereupon', 'indeed', 'without', 'us', 'up', 'un', 'as', 'hasnt', 'what', 'least', 'per', 'her', 'until', 'hereafter', 'everywhere', 'sometimes', 'inc', 'together', 'several', 'cannot', 'him', 'seemed', 'front', 'serious', 'done', 'de', 'hence', 'to', 'only', 'have', 'thru', 'part', 'onto', 'could', 'amongst', 'of', 'another', 'became', 'hundred', 'how', 'within', 'three', 'behind', 'third', 'too', 'wherein', 'anyway', 'through', 'although', 'becoming', 'herself', 'former', 'will', 'along', 'latterly', 'hereby', 'when', 'they', 'cry', 'whence', 'anything', 'herein', 'below', 'well', 'your', 'interest', 'noone', 'also', 'twelve', 'still', 'found', 'ourselves', 'me', 'once', 'name', 'upon', 'show', 'other', 'about', 'nine', 'perhaps', 'its', 'alone', 'she', 'above', 'bill', 'very', 'etc', 'some', 'seem', 'his', 'than', 'get', 'fifteen', 'thus', 'that', 'himself', 'are', 'eleven', 'amount', 'whom', 'wherever', 'nor', 'hereupon', 'thin', 'yourself', 'ltd', 'twenty', 'though', 'i', 'do', 'somehow', 'sixty', 'four', 'made', 'beforehand', 'fill', 'therein', 'yet', 'thereby', 'otherwise', 'mill', 'amoungst', 'neither', 'if', 'an', 'a', 'were', 'on', 'beyond', 'less', 'against', 'after', 'there', 'now', 'mine', 'top', 'out', 'mostly', 'between', 'both', 'due', 'same', 'which', 'at', 'because', 'no', 'system', 'was', 'throughout', 'who', 'itself', 'themselves', 'whole', 'go', 'down', 'describe', 'sometime', 'before', 'others', 'one', 'always', 'he', 'but', 'give', 'during', 'may', 'them', 'call', 'anywhere', 'back', 'eight', 'again', 'via', 'am', 'ie', 'often', 'has', 'someone', 'ever', 'you', 'among', 'for', 'take', 'be', 'and', 'seems', 'co', 'seeming', 'why', 'besides', 'elsewhere', 'around', 'whose', 'see', 'not', 'toward', 'already', 'own', 'thick', 'whereupon', 'never', 'had', 'every', 'move', 'my', 'except', 'into', 'meanwhile', 'fifty', 'this', 'latter'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'ever', 'thence', 'others', 'formerly', 'call', 'somewhere', 'twenty', 'might', 'last', 'hence', 'wherever', 'about', 'anywhere', 'everywhere', 'can', 'neither', 'towards', 'front', 'cannot', 'both', 'once', 'himself', 'five', 'latter', 'via', 'another', 'our', 'amongst', 'which', 'above', 'us', 'while', 'part', 'whether', 'every', 'six', 'yourself', 'been', 'under', 'such', 'together', 'else', 'hundred', 'made', 'afterwards', 'whatever', 'give', 'go', 'three', 'however', 'for', 'though', 'none', 'may', 'themselves', 'other', 'seemed', 'top', 'now', 'thereby', 'or', 'sincere', 'are', 'any', 'describe', 'they', 'then', 'everything', 'further', 'amount', 'two', 'whole', 'it', 'whereupon', 'keep', 'per', 'nevertheless', 'but', 'of', 'she', 'see', 'several', 'myself', 'cry', 'many', 'hereafter', 'although', 'bill', 'sixty', 'latterly', 'done', 'former', 'perhaps', 'against', 'due', 'side', 'hasnt', 'even', 'who', 'than', 'around', 'with', 'name', 'across', 'him', 'is', 'by', 'always', 'whoever', 'his', 'much', 'all', 'anyway', 'toward', 'fifteen', 'anyhow', 'has', 'would', 'seeming', 'anyone', 'therein', 'full', 'please', 'am', 'beforehand', 'serious', 're', 'nine', 'same', 'enough', 'how', 'often', 'inc', 'again', 'do', 'because', 'anything', 'alone', 'get', 'them', 'between', 'ourselves', 'become', 'had', 'move', 'those', 'very', 'and', 'i', 'system', 'thin', 'seems', 'up', 'in', 'a', 'ie', 'so', 'next', 'beside', 'until', 'something', 'moreover', 'first', 'without', 'off', 'among', 'nobody', 'found', 'whereafter', 'becoming', 'few', 'mine', 'you', 'me', 'twelve', 'interest', 'he', 'beyond', 'also', 'take', 'behind', 'these', 'least', 'well', 'four', 'meanwhile', 'someone', 'everyone', 'etc', 'still', 'thru', 'this', 'most', 'sometime', 'find', 'detail', 'therefore', 'whereby', 'except', 'eight', 'be', 'nothing', 'to', 'before', 'thereafter', 'whenever', 'bottom', 'hereby', 'within', 'co', 'wherein', 'eg', 'being', 'their', 'itself', 'hereupon', 'what', 'ours', 'thick', 'mill', 'each', 'noone', 'one', 'should', 'when', 'no', 'forty', 'namely', 'will', 'the', 'here', 'onto', 'besides', 'your', 'on', 'whereas', 'whose', 'de', 'why', 'fill', 'if', 'herein', 'yet', 'con', 'seem', 'more', 'somehow', 'rather', 'must', 'elsewhere', 'an', 'too', 'into', 'was', 'were', 'show', 'we', 'after', 'cant', 'since', 'through', 'never', 'not', 'fire', 'during', 'ten', 'yours', 'could', 'empty', 'from', 'put', 'thus', 'whom', 'became', 'whither', 'thereupon', 'less', 'un', 'indeed', 'mostly', 'throughout', 'out', 'own', 'nor', 'hers', 'where', 'that', 'back', 'some', 'only', 'my', 'sometimes', 'third', 'upon', 'herself', 'already', 'fifty', 'as', 'its', 'nowhere', 'ltd', 'yourselves', 'otherwise', 'almost', 'at', 'there', 'down', 'becomes', 'couldnt', 'either', 'amoungst', 'over', 'eleven', 'have', 'along', 'below', 'her', 'whence'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'same', 'or', 'see', 'whatever', 'a', 'they', 'than', 'hereafter', 'meanwhile', 'if', 'over', 'whence', 'again', 'well', 'cannot', 'bill', 'ours', 'whither', 'everywhere', 'along', 'move', 'ever', 'off', 'twenty', 'interest', 'against', 'one', 'keep', 'any', 'sixty', 'three', 'cry', 'all', 'could', 'anyone', 'also', 'anyhow', 'although', 'him', 'next', 'upon', 'none', 'up', 'serious', 'another', 'my', 'de', 'themselves', 'how', 'everything', 'thru', 'fill', 'your', 'myself', 'forty', 'an', 'himself', 'four', 'whoever', 'should', 'down', 'side', 'everyone', 'our', 'nowhere', 'seeming', 'third', 'eleven', 'by', 'since', 'whereas', 'within', 'seem', 'us', 'take', 'nevertheless', 'somehow', 'formerly', 'might', 'less', 'latterly', 'which', 'every', 'would', 'still', 'between', 'about', 'across', 'had', 'thick', 'through', 'fifteen', 'whereby', 'get', 'anything', 'find', 'whereafter', 'become', 'herein', 'several', 'seems', 'we', 'around', 'somewhere', 'mill', 'ten', 'that', 'besides', 'beforehand', 'co', 'hasnt', 'please', 'ie', 'without', 'amongst', 'once', 'made', 'you', 'under', 'in', 'rather', 'couldnt', 'only', 'hundred', 'do', 'always', 'here', 'afterwards', 'each', 'after', 'fifty', 'together', 'already', 'some', 'when', 'hence', 'never', 'this', 'system', 'throughout', 'two', 'both', 'onto', 'con', 'while', 'describe', 'go', 'noone', 'twelve', 'from', 'was', 'therein', 'because', 'among', 'etc', 'anywhere', 'yourself', 'may', 'for', 'bottom', 'former', 'per', 'became', 'indeed', 'much', 'beyond', 'yet', 'behind', 'eg', 'detail', 'thereby', 'least', 'most', 'ourselves', 'until', 'were', 'with', 'even', 'nobody', 'ltd', 'further', 'whole', 'his', 'amount', 'however', 'sometime', 'being', 'show', 'at', 'often', 'done', 'becomes', 'above', 'towards', 'itself', 'hereupon', 'either', 'of', 'something', 'and', 'have', 'no', 'whereupon', 'elsewhere', 'becoming', 'where', 'whenever', 'sincere', 'can', 'be', 'on', 'mine', 'yours', 'thereupon', 'but', 'hers', 'full', 'enough', 'otherwise', 'found', 'them', 'amoungst', 'these', 'yourselves', 'must', 'below', 'not', 'am', 'fire', 'it', 'he', 'perhaps', 'toward', 'thence', 'more', 'very', 'un', 'due', 'been', 'such', 'now', 'who', 'its', 'so', 'namely', 'sometimes', 'thereafter', 're', 'eight', 'me', 'other', 'those', 'are', 'nothing', 'whose', 'give', 'the', 'inc', 'seemed', 'someone', 'put', 'i', 'alone', 'latter', 'what', 'many', 'out', 'thin', 'whom', 'except', 'herself', 'will', 'first', 'beside', 'she', 'therefore', 'else', 'her', 'cant', 'during', 'few', 'via', 'why', 'wherein', 'is', 'as', 'last', 'though', 'moreover', 'whether', 'call', 'others', 'mostly', 'empty', 'part', 'hereby', 'own', 'wherever', 'nine', 'five', 'their', 'top', 'nor', 'before', 'into', 'back', 'name', 'almost', 'too', 'front', 'then', 'six', 'neither', 'to', 'anyway', 'there', 'has', 'thus'}), vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11f207ce0>; total time=  55.5s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x12ca16700>; total time=  56.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:516: FitFailedWarning: \n",
      "50 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 655, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 589, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2105, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'tokenizer' parameter of TfidfVectorizer must be a callable or None. Got ['runner', 'like', 'run', 'and', 'thu', 'they', 'run'] instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 655, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 589, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2105, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'con', 'beforehand', 'show', 'for', 'over', 'empty', 'more', 'thin', 'four', 'towards', 'herein', 'hence', 'until', 'among', 'will', 'become', 'former', 'next', 'thick', 'whole', 'whereas', 'never', 'hereafter', 'everything', 'is', 'our', 'during', 'while', 'than', 'their', 'it', 'others', 'that', 'down', 'latterly', 'fill', 'herself', 'whenever', 'his', 'ltd', 'eg', 'several', 'side', 'since', 'my', 'your', 'see', 'up', 'when', 'are', 'becomes', 'every', 'were', 'everywhere', 'would', 'indeed', 'yet', 'interest', 'ten', 'namely', 'couldnt', 'elsewhere', 'anything', 'seemed', 'of', 'name', 'you', 'becoming', 'move', 'very', 'third', 'nine', 'whom', 'around', 'sometime', 'beyond', 'wherever', 'twenty', 'seems', 'too', 'below', 'forty', 'between', 'who', 'most', 'thence', 'behind', 'etc', 'otherwise', 'once', 'have', 'those', 'above', 'be', 'we', 'only', 'to', 'alone', 'off', 'full', 'nobody', 'hasnt', 'eight', 'he', 'hundred', 'un', 'fifteen', 'became', 'before', 'first', 'one', 'seem', 'the', 'ie', 'am', 'whose', 'some', 're', 'whence', 'thereupon', 'now', 'can', 'meanwhile', 'call', 'whereby', 'itself', 'however', 'being', 'then', 'enough', 'last', 'as', 'within', 'on', 'none', 'still', 'almost', 'co', 'thus', 'across', 'many', 'besides', 'moreover', 'must', 'under', 'also', 'anywhere', 'hereby', 'hers', 'get', 'bill', 'twelve', 'even', 'amoungst', 'which', 'was', 'do', 'well', 'de', 'they', 'himself', 'these', 'perhaps', 'serious', 'her', 'us', 'its', 'without', 'if', 'might', 'how', 'back', 'someone', 'why', 'ourselves', 'whoever', 'fifty', 'whereafter', 'other', 'neither', 'themselves', 'but', 'via', 'an', 'or', 'each', 'yours', 'hereupon', 'six', 'nothing', 'any', 'either', 'not', 'nevertheless', 'where', 'rather', 'put', 'detail', 'may', 'along', 'nowhere', 'made', 'should', 'anyway', 'has', 'sixty', 'system', 'done', 'anyhow', 'what', 'cry', 'latter', 'both', 'three', 'she', 'found', 'in', 'often', 'therein', 'beside', 'all', 'therefore', 'much', 'sometimes', 'out', 'top', 'per', 'by', 'noone', 'give', 'part', 'least', 'ours', 'here', 'cant', 'and', 'yourselves', 'else', 'so', 'yourself', 'into', 'due', 'mill', 'with', 'a', 'me', 'upon', 'after', 'together', 'anyone', 'always', 'mine', 'thru', 'again', 'had', 'myself', 'thereby', 'against', 'although', 'two', 'whether', 'amongst', 'been', 'bottom', 'afterwards', 'own', 'ever', 'throughout', 'nor', 'whereupon', 'another', 'find', 'except', 'this', 'please', 'somewhere', 'seeming', 'five', 'go', 'toward', 'at', 'take', 'because', 'fire', 'thereafter', 'mostly', 'somehow', 'no', 'keep', 'amount', 'them', 'whatever', 'everyone', 'i', 'eleven', 'whither', 'though', 'formerly', 'wherein', 'describe', 'same', 'onto', 'through', 'further', 'from', 'sincere', 'less', 'there', 'him', 'inc', 'about', 'already', 'such', 'something', 'could', 'cannot', 'few', 'front'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 655, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 589, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2105, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'there', 'becomes', 'became', 'top', 'without', 'namely', 'onto', 'hers', 'more', 'behind', 'whose', 'bottom', 'have', 'into', 'his', 'are', 'than', 'that', 'who', 'fire', 'such', 'empty', 'how', 'five', 'per', 'beside', 'thick', 'name', 'should', 'first', 'after', 'whither', 'therein', 'had', 'few', 'nothing', 'every', 'others', 'fifty', 'yourselves', 'must', 'your', 'no', 'done', 'forty', 'now', 'her', 'everywhere', 'i', 'on', 'toward', 'thereupon', 'several', 'ten', 'my', 'same', 'almost', 'hereby', 'back', 'inc', 'show', 'throughout', 'yet', 'is', 'within', 'mill', 'describe', 'front', 'both', 'keep', 'four', 'these', 'next', 'might', 'together', 'herself', 'whence', 'besides', 'above', 'nobody', 'take', 'found', 'mostly', 'everyone', 'were', 'otherwise', 'co', 'please', 'except', 'three', 'however', 'via', 'upon', 'our', 'it', 'enough', 'to', 'yourself', 'wherein', 'during', 'we', 'seeming', 'therefore', 'find', 'many', 'because', 'hasnt', 'ourselves', 'whoever', 'wherever', 'six', 'own', 'move', 'may', 'thru', 'would', 'something', 'further', 'yours', 'whereby', 'see', 'but', 'amoungst', 'where', 'most', 'under', 'get', 'sometimes', 'hereafter', 'con', 'least', 'anywhere', 'has', 'un', 'ltd', 'them', 'noone', 'then', 'us', 'meanwhile', 'whatever', 'anyhow', 'well', 'while', 'so', 'themselves', 'down', 'you', 'twelve', 'some', 'too', 'rather', 'moreover', 'if', 'seem', 'afterwards', 'along', 'eight', 'seemed', 'any', 'cant', 'between', 'this', 'do', 'whereafter', 'already', 'whom', 'once', 'beyond', 'beforehand', 'myself', 'the', 'formerly', 'thence', 'sincere', 'though', 'am', 'elsewhere', 'from', 'ever', 'perhaps', 'call', 'again', 'fifteen', 'what', 'sometime', 'thereby', 'interest', 'fill', 're', 'former', 'him', 'they', 'less', 'not', 'everything', 'nevertheless', 'across', 'always', 'up', 'or', 'off', 'before', 'by', 'made', 'alone', 'thin', 'system', 'other', 'me', 'give', 'cannot', 'due', 'below', 'eg', 'neither', 'of', 'can', 'for', 'also', 'twenty', 'an', 'among', 'even', 'much', 'being', 'could', 'hence', 'with', 'himself', 'someone', 'thus', 'amongst', 'when', 'through', 'one', 'couldnt', 'each', 'against', 'becoming', 'in', 'whenever', 'will', 'anyone', 'detail', 'latterly', 'somehow', 'thereafter', 'never', 'de', 'nor', 'only', 'full', 'here', 'was', 'put', 'still', 'somewhere', 'and', 'he', 'often', 'go', 'serious', 'seems', 'she', 'itself', 'last', 'their', 'all', 'cry', 'since', 'part', 'ie', 'nowhere', 'whether', 'as', 'another', 'at', 'towards', 'hereupon', 'whole', 'whereupon', 'side', 'those', 'out', 'latter', 'hundred', 'etc', 'anyway', 'indeed', 'bill', 'been', 'amount', 'its', 'be', 'which', 'third', 'become', 'why', 'nine', 'although', 'over', 'about', 'herein', 'around', 'anything', 'eleven', 'very', 'sixty', 'none', 'two', 'until', 'ours', 'mine', 'either', 'whereas', 'a', 'else'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 655, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 589, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2105, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'same', 'or', 'see', 'whatever', 'a', 'they', 'than', 'hereafter', 'meanwhile', 'if', 'over', 'whence', 'again', 'well', 'cannot', 'bill', 'ours', 'whither', 'everywhere', 'along', 'move', 'ever', 'off', 'twenty', 'interest', 'against', 'one', 'keep', 'any', 'sixty', 'three', 'cry', 'all', 'could', 'anyone', 'also', 'anyhow', 'although', 'him', 'next', 'upon', 'none', 'up', 'serious', 'another', 'my', 'de', 'themselves', 'how', 'everything', 'thru', 'fill', 'your', 'myself', 'forty', 'an', 'himself', 'four', 'whoever', 'should', 'down', 'side', 'everyone', 'our', 'nowhere', 'seeming', 'third', 'eleven', 'by', 'since', 'whereas', 'within', 'seem', 'us', 'take', 'nevertheless', 'somehow', 'formerly', 'might', 'less', 'latterly', 'which', 'every', 'would', 'still', 'between', 'about', 'across', 'had', 'thick', 'through', 'fifteen', 'whereby', 'get', 'anything', 'find', 'whereafter', 'become', 'herein', 'several', 'seems', 'we', 'around', 'somewhere', 'mill', 'ten', 'that', 'besides', 'beforehand', 'co', 'hasnt', 'please', 'ie', 'without', 'amongst', 'once', 'made', 'you', 'under', 'in', 'rather', 'couldnt', 'only', 'hundred', 'do', 'always', 'here', 'afterwards', 'each', 'after', 'fifty', 'together', 'already', 'some', 'when', 'hence', 'never', 'this', 'system', 'throughout', 'two', 'both', 'onto', 'con', 'while', 'describe', 'go', 'noone', 'twelve', 'from', 'was', 'therein', 'because', 'among', 'etc', 'anywhere', 'yourself', 'may', 'for', 'bottom', 'former', 'per', 'became', 'indeed', 'much', 'beyond', 'yet', 'behind', 'eg', 'detail', 'thereby', 'least', 'most', 'ourselves', 'until', 'were', 'with', 'even', 'nobody', 'ltd', 'further', 'whole', 'his', 'amount', 'however', 'sometime', 'being', 'show', 'at', 'often', 'done', 'becomes', 'above', 'towards', 'itself', 'hereupon', 'either', 'of', 'something', 'and', 'have', 'no', 'whereupon', 'elsewhere', 'becoming', 'where', 'whenever', 'sincere', 'can', 'be', 'on', 'mine', 'yours', 'thereupon', 'but', 'hers', 'full', 'enough', 'otherwise', 'found', 'them', 'amoungst', 'these', 'yourselves', 'must', 'below', 'not', 'am', 'fire', 'it', 'he', 'perhaps', 'toward', 'thence', 'more', 'very', 'un', 'due', 'been', 'such', 'now', 'who', 'its', 'so', 'namely', 'sometimes', 'thereafter', 'eight', 're', 'me', 'other', 'those', 'are', 'nothing', 'whose', 'give', 'the', 'inc', 'seemed', 'someone', 'put', 'i', 'alone', 'latter', 'what', 'many', 'out', 'thin', 'whom', 'except', 'herself', 'will', 'first', 'beside', 'she', 'therefore', 'else', 'her', 'cant', 'during', 'few', 'via', 'why', 'wherein', 'is', 'as', 'last', 'though', 'moreover', 'whether', 'call', 'others', 'mostly', 'empty', 'part', 'hereby', 'own', 'wherever', 'nine', 'five', 'their', 'top', 'nor', 'before', 'into', 'back', 'name', 'almost', 'too', 'front', 'then', 'six', 'neither', 'to', 'anyway', 'there', 'has', 'thus'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 655, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 589, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2105, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'ever', 'thence', 'others', 'formerly', 'call', 'somewhere', 'twenty', 'might', 'last', 'hence', 'wherever', 'about', 'anywhere', 'everywhere', 'can', 'neither', 'towards', 'front', 'cannot', 'both', 'once', 'himself', 'five', 'latter', 'via', 'another', 'our', 'amongst', 'which', 'above', 'us', 'while', 'part', 'whether', 'every', 'six', 'yourself', 'been', 'under', 'such', 'together', 'else', 'hundred', 'made', 'afterwards', 'whatever', 'give', 'go', 'three', 'however', 'for', 'though', 'none', 'may', 'themselves', 'other', 'seemed', 'top', 'now', 'thereby', 'or', 'sincere', 'are', 'any', 'describe', 'they', 'then', 'everything', 'further', 'amount', 'two', 'whole', 'it', 'whereupon', 'keep', 'per', 'nevertheless', 'but', 'of', 'she', 'see', 'several', 'myself', 'cry', 'many', 'hereafter', 'although', 'bill', 'sixty', 'latterly', 'done', 'former', 'perhaps', 'against', 'due', 'side', 'hasnt', 'even', 'who', 'than', 'around', 'with', 'name', 'across', 'him', 'is', 'by', 'always', 'whoever', 'his', 'much', 'all', 'anyway', 'toward', 'fifteen', 'anyhow', 'has', 'would', 'seeming', 'anyone', 'therein', 'full', 'please', 'am', 'beforehand', 'serious', 're', 'nine', 'same', 'enough', 'how', 'often', 'inc', 'again', 'do', 'because', 'anything', 'alone', 'get', 'them', 'between', 'ourselves', 'become', 'had', 'move', 'those', 'very', 'and', 'i', 'system', 'thin', 'seems', 'up', 'in', 'a', 'ie', 'so', 'next', 'beside', 'until', 'something', 'moreover', 'first', 'without', 'off', 'among', 'nobody', 'found', 'whereafter', 'becoming', 'few', 'mine', 'you', 'me', 'twelve', 'interest', 'he', 'beyond', 'also', 'take', 'behind', 'these', 'least', 'well', 'four', 'meanwhile', 'someone', 'everyone', 'etc', 'still', 'thru', 'this', 'most', 'sometime', 'find', 'detail', 'therefore', 'whereby', 'except', 'eight', 'be', 'nothing', 'to', 'before', 'thereafter', 'whenever', 'bottom', 'hereby', 'within', 'co', 'wherein', 'eg', 'being', 'their', 'itself', 'hereupon', 'what', 'ours', 'thick', 'mill', 'each', 'noone', 'one', 'should', 'when', 'no', 'forty', 'namely', 'will', 'the', 'here', 'onto', 'besides', 'your', 'on', 'whereas', 'whose', 'de', 'why', 'fill', 'if', 'herein', 'yet', 'con', 'seem', 'more', 'somehow', 'rather', 'must', 'elsewhere', 'an', 'too', 'into', 'was', 'were', 'show', 'we', 'after', 'cant', 'since', 'through', 'never', 'not', 'fire', 'during', 'ten', 'yours', 'could', 'empty', 'from', 'put', 'thus', 'whom', 'became', 'whither', 'thereupon', 'less', 'un', 'indeed', 'mostly', 'throughout', 'out', 'own', 'nor', 'hers', 'where', 'that', 'back', 'some', 'only', 'my', 'sometimes', 'third', 'upon', 'herself', 'already', 'fifty', 'as', 'its', 'nowhere', 'ltd', 'yourselves', 'otherwise', 'almost', 'at', 'there', 'down', 'becomes', 'couldnt', 'either', 'amoungst', 'over', 'eleven', 'have', 'along', 'below', 'her', 'whence'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 655, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 589, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2105, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'keep', 'whither', 'however', 'whereby', 'or', 'off', 'such', 'since', 'find', 'being', 'might', 'most', 'must', 'sincere', 'formerly', 're', 'many', 'something', 'rather', 'more', 'nothing', 'side', 'namely', 'our', 'each', 'over', 'these', 'thereafter', 'first', 'any', 'enough', 'nowhere', 'become', 'under', 'two', 'five', 'thence', 'everyone', 'whereafter', 'bottom', 'so', 'beside', 'with', 'none', 'where', 'should', 'all', 'been', 'full', 'yours', 'across', 'here', 'whenever', 'hers', 'few', 'couldnt', 'myself', 'detail', 'anyhow', 'last', 'even', 'six', 'further', 'afterwards', 'cant', 'moreover', 'we', 'can', 'while', 'much', 'forty', 'else', 'eg', 'please', 'next', 'would', 'anyone', 'whatever', 'in', 'towards', 'the', 'fire', 'yourselves', 'everything', 'put', 'by', 'it', 'then', 'therefore', 'from', 'empty', 'whether', 'either', 'nobody', 'almost', 'those', 'becomes', 'ten', 'nevertheless', 'ours', 'con', 'their', 'somewhere', 'whereas', 'is', 'whoever', 'thereupon', 'indeed', 'without', 'us', 'up', 'un', 'as', 'hasnt', 'what', 'least', 'per', 'her', 'until', 'hereafter', 'everywhere', 'sometimes', 'inc', 'together', 'several', 'cannot', 'him', 'seemed', 'front', 'serious', 'done', 'de', 'hence', 'to', 'only', 'have', 'thru', 'part', 'onto', 'could', 'amongst', 'of', 'another', 'became', 'hundred', 'how', 'within', 'three', 'behind', 'third', 'too', 'wherein', 'anyway', 'through', 'although', 'becoming', 'herself', 'former', 'will', 'along', 'latterly', 'hereby', 'when', 'they', 'cry', 'whence', 'anything', 'herein', 'below', 'well', 'your', 'interest', 'noone', 'also', 'twelve', 'still', 'found', 'ourselves', 'me', 'once', 'name', 'upon', 'show', 'other', 'about', 'nine', 'perhaps', 'its', 'alone', 'she', 'above', 'bill', 'very', 'etc', 'some', 'seem', 'his', 'than', 'get', 'fifteen', 'thus', 'that', 'himself', 'are', 'eleven', 'amount', 'whom', 'wherever', 'nor', 'hereupon', 'thin', 'yourself', 'ltd', 'twenty', 'though', 'i', 'do', 'somehow', 'sixty', 'four', 'made', 'beforehand', 'fill', 'therein', 'yet', 'thereby', 'otherwise', 'mill', 'amoungst', 'neither', 'if', 'an', 'a', 'were', 'on', 'beyond', 'less', 'against', 'after', 'there', 'now', 'mine', 'top', 'out', 'mostly', 'between', 'both', 'due', 'same', 'which', 'at', 'because', 'no', 'system', 'was', 'throughout', 'who', 'itself', 'themselves', 'whole', 'go', 'down', 'describe', 'sometime', 'before', 'others', 'one', 'always', 'he', 'but', 'give', 'during', 'may', 'them', 'call', 'anywhere', 'back', 'eight', 'again', 'via', 'am', 'ie', 'often', 'has', 'someone', 'ever', 'you', 'among', 'for', 'take', 'be', 'and', 'seems', 'co', 'seeming', 'why', 'besides', 'elsewhere', 'around', 'whose', 'see', 'not', 'toward', 'already', 'own', 'thick', 'whereupon', 'never', 'had', 'every', 'move', 'my', 'except', 'into', 'meanwhile', 'fifty', 'this', 'latter'}) instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1135: UserWarning: One or more of the test scores are non-finite: [ nan 0.89  nan 0.89  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "  warnings.warn(\n",
      "/Users/sarthakbiswas/projects/ai/sentiment-analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
       "                          &#x27;vect__stop_words&#x27;: [None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [[&#x27;runner&#x27;, &#x27;like&#x27;, &#x27;run&#x27;, &#x27;and&#x27;,\n",
       "                                               &#x27;thu&#x27;, &#x27;they&#x27;, &#x27;run&#x27;],\n",
       "                                              &lt;function tokenizer_porter at 0...\n",
       "                                                          &#x27;afterwards&#x27;, &#x27;again&#x27;,\n",
       "                                                          &#x27;against&#x27;, &#x27;all&#x27;,\n",
       "                                                          &#x27;almost&#x27;, &#x27;alone&#x27;,\n",
       "                                                          &#x27;along&#x27;, &#x27;already&#x27;,\n",
       "                                                          &#x27;also&#x27;, &#x27;although&#x27;,\n",
       "                                                          &#x27;always&#x27;, &#x27;am&#x27;,\n",
       "                                                          &#x27;among&#x27;, &#x27;amongst&#x27;,\n",
       "                                                          &#x27;amoungst&#x27;, &#x27;amount&#x27;,\n",
       "                                                          &#x27;an&#x27;, &#x27;and&#x27;,\n",
       "                                                          &#x27;another&#x27;, &#x27;any&#x27;,\n",
       "                                                          &#x27;anyhow&#x27;, &#x27;anyone&#x27;,\n",
       "                                                          &#x27;anything&#x27;, &#x27;anyway&#x27;,\n",
       "                                                          &#x27;anywhere&#x27;, ...}),\n",
       "                                               None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [[&#x27;runner&#x27;, &#x27;like&#x27;, &#x27;run&#x27;, &#x27;and&#x27;,\n",
       "                                               &#x27;thu&#x27;, &#x27;they&#x27;, &#x27;run&#x27;]],\n",
       "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('estimator',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">estimator&nbsp;</td>\n",
       "            <td class=\"value\">Pipeline(step...liblinear&#x27;))])</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('param_grid',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">param_grid&nbsp;</td>\n",
       "            <td class=\"value\">[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;], &#x27;vect__ngram_range&#x27;: [(1, ...)], &#x27;vect__stop_words&#x27;: [None], ...}, {&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;, &#x27;l1&#x27;], &#x27;vect__ngram_range&#x27;: [(1, ...)], &#x27;vect__norm&#x27;: [None], ...}]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('scoring',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">scoring&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;accuracy&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('refit',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">refit&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cv',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">cv&nbsp;</td>\n",
       "            <td class=\"value\">5</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">2</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('pre_dispatch',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">pre_dispatch&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;2*n_jobs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('error_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">error_score&nbsp;</td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('return_train_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">return_train_score&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: Pipeline</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___\"></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TfidfVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___vect__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('input',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">input&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;content&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('encoding',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">encoding&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;utf-8&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('decode_error',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">decode_error&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;strict&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('strip_accents',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">strip_accents&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('lowercase',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">lowercase&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('preprocessor',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">preprocessor&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tokenizer',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tokenizer&nbsp;</td>\n",
       "            <td class=\"value\">&lt;function tok...t 0x12bd56160&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('analyzer',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">analyzer&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;word&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('stop_words',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">stop_words&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('token_pattern',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">token_pattern&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;(?u)\\\\b\\\\w\\\\w+\\\\b&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ngram_range',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ngram_range&nbsp;</td>\n",
       "            <td class=\"value\">(1, ...)</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_df',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_df&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_df',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_df&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_features',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_features&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('vocabulary',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">vocabulary&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('binary',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">binary&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dtype',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dtype&nbsp;</td>\n",
       "            <td class=\"value\">&lt;class &#x27;numpy.float64&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('norm',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">norm&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('use_idf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">use_idf&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('smooth_idf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">smooth_idf&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sublinear_tf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sublinear_tf&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___clf__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">penalty&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dual',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dual&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">10.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fit_intercept&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('intercept_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">intercept_scaling&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('solver',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">solver&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;liblinear&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">100</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_class',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_class&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">l1_ratio&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'],\n",
       "                          'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokenizer': [['runner', 'like', 'run', 'and',\n",
       "                                               'thu', 'they', 'run'],\n",
       "                                              <function tokenizer_porter at 0...\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...}),\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [['runner', 'like', 'run', 'and',\n",
       "                                               'thu', 'they', 'run']],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# feature extraction \n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "# hyperparameter tuning \n",
    "small_param_grid = [\n",
    "    {\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [None],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1.0, 10.0]\n",
    "    },\n",
    "    {\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer],\n",
    "        'vect__use_idf':[False],\n",
    "        'vect__norm':[None],\n",
    "        'clf__penalty': ['l2', 'l1'],\n",
    "        'clf__C': [1.0, 10.0]\n",
    "    },\n",
    "]\n",
    "\n",
    "# pipeline \n",
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "# running grid searchCV \n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
    "                           scoring='accuracy', cv=5,\n",
    "                           verbose=2, n_jobs=-1)\n",
    "\n",
    "\n",
    "# fit the model \n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e894a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_porter at 0x12bd56160>}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d29c73d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.892\n"
     ]
    }
   ],
   "source": [
    "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8870fffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.894\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be579ac3",
   "metadata": {},
   "source": [
    "using SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c86066e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/var/folders/8t/5rdykr5x1sb_n85_5z0tjcr80000gn/T/ipykernel_37117/240731717.py:13: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
      "/var/folders/8t/5rdykr5x1sb_n85_5z0tjcr80000gn/T/ipykernel_37117/240731717.py:15: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  text = re.sub('[\\W]+', ' ', text.lower()) \\\n"
     ]
    }
   ],
   "source": [
    "# remove stop words \n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "stop = ENGLISH_STOP_WORDS\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                  + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b17ce6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns docs one by one \n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "            \n",
    "# check the function is working or not \n",
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f518492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form mini batches from the doc stream \n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27a28b",
   "metadata": {},
   "source": [
    "- can't use countVectorizer for out of core learning because \n",
    "  it requires holding the complete vocabulary in memory.\n",
    "- Also, TfidfVectorizer needs to keep all the feature vectors \n",
    "  of the training dataset in memory to calculate the inverse document frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9c71542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log_loss', random_state=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f760519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:06\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45, stream=sys.stdout)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e26a5d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(f'Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb17e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98d7ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "# the following is necessary on some computers:\n",
    "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "052d6438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a32b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a06c7d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58d3fe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes awful script stupid\n",
      "Topic 2:\n",
      "family mother father children girl\n",
      "Topic 3:\n",
      "american war dvd music tv\n",
      "Topic 4:\n",
      "human audience cinema art sense\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house sex girl woman\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode war episodes tv\n",
      "Topic 9:\n",
      "book version original read novel\n",
      "Topic 10:\n",
      "action fight guy guys cool\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {(topic_idx + 1)}:')\n",
    "    print(' '.join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                    [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a960d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "\n",
      "Horror movie #2:\n",
      "Okay, what the hell kind of TRASH have I been watching now? \"The Witches' Mountain\" has got to be one of the most incoherent and insane Spanish exploitation flicks ever and yet, at the same time, it's also strangely compelling. There's absolutely nothing that makes sense here and I even doubt there  ...\n",
      "\n",
      "Horror movie #3:\n",
      "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print(f'\\nHorror movie #{(iter_idx + 1)}:')\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
